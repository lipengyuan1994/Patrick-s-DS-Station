{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "%logstop\n",
    "%logstart -rtq ~/.logs/DS_IO.py append\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "matplotlib.rcParams['figure.dpi'] = 144"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import expectexception"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing and Exporting Data\n",
    "\n",
    "<!-- requirement: data/sample.txt -->\n",
    "<!-- requirement: data/csv_sample.txt -->\n",
    "<!-- requirement: data/bad_csv.csv -->\n",
    "\n",
    "So far we've only dealt with data that we have created within Python. Generating random data is helpful for testing out ideas, but we want to work with real data. Most often that data will be stored in a file, either locally on the computer or online. In this notebook we'll learn how to read and write data to files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python file handles (`open`)\n",
    "\n",
    "In Python we interact with files on disk using the commands `open` and `close`. We've included a file in the `data` folder called `sample.txt`. Let's open it and read its contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('./data/sample.txt', 'r')\n",
    "data = f.read()\n",
    "f.close()\n",
    "\n",
    "print(data)\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we `open` the file  and assign it to `f`, `read` the data from `f`, and then close `f`. What is `f`? It's called a **file handle**. It's an object that connects Python to the file we `open`. We `read` the data using this connection, and then once we're done with `close` the connection. It's a good habit to `close` a file handle once we're done with it, so usually we will do it automatically using Python's `with` keyword. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f is automatically closed\n",
    "# at the end of the body of the with statement\n",
    "with open('./data/sample.txt', 'r') as f:\n",
    "    print(f.read())\n",
    "\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also read individual lines of a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/sample.txt', 'r') as f:\n",
    "    print(f.readline())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/sample.txt', 'r') as f:\n",
    "    print(f.readlines())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing data to files is very similar. The main difference is when we `open` the file, we will use the `'w'` flag instead of `'r'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/my_data.txt', 'w') as f:\n",
    "    f.write('This is a new file.')\n",
    "    f.write('I am practicing writing data to disk.')\n",
    "\n",
    "with open('./data/my_data.txt', 'r') as f:\n",
    "    my_data = f.read()\n",
    "\n",
    "print(my_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No matter how often I execute the above cell, the same output gets printed. Opening the file with the `'w'` flag will overwrite the contents of the file. If we want to add to what is already in the file, we have to open the file with the `'a'` flag (`'a'` stands for _append_)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/my_data.txt', 'a') as f:\n",
    "    f.write('\\nAdding a new line to the file.')\n",
    "\n",
    "with open('./data/my_data.txt', 'r') as f:\n",
    "    my_data = f.read()\n",
    "\n",
    "print(my_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We always need to be careful when writing to disk, because we could overwrite or alter data by accident. It is also easy to encounter errors when working with files, because we might not know ahead of time if the file we're trying to access exists, or we might mix up the `'r'`, `'w'`, and `'a'` flags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%expect_exception IOError\n",
    "\n",
    "# if a file doesn't exist\n",
    "# we can't open it for reading\n",
    "# (but we can open it for writing)\n",
    "\n",
    "with open('./data/fail.txt', 'r') as f:\n",
    "    f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%expect_exception IOError\n",
    "\n",
    "# we can't read a file open for writing\n",
    "\n",
    "with open('./data/fail.txt', 'w') as f:\n",
    "    f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%expect_exception IOError\n",
    "\n",
    "# and we can't write to a file open for reading\n",
    "\n",
    "with open('./data/sample.txt', 'r') as f:\n",
    "    f.write('This will fail')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we prevent some of these errors? How do we find out what files are on disk?\n",
    "\n",
    "## `os` module\n",
    "\n",
    "Python has a module for navigating the computer's file system called `os`. There are many useful tools in the `os` module, but there are two functions that are most useful for finding files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DS_IO.ipynb',\n",
       " '.done',\n",
       " 'DS_SQL.ipynb',\n",
       " 'data',\n",
       " 'DS_Basic_DS_Modules.ipynb',\n",
       " '.ipynb_checkpoints',\n",
       " 'DS_Classes_and_ORM.ipynb',\n",
       " 'DS_Pandas.ipynb',\n",
       " 'DS_Data_Munging.ipynb',\n",
       " 'miniprojects',\n",
       " 'DS_Intro_Statistics.ipynb']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# list the contents of the current directory\n",
    "# ('.' refers to the current directory)\n",
    "os.listdir('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The command `listdir` is the simpler of the two functions we'll cover. It simply lists the contents of the directory path we specify. When we pass `'.'` as the argument, `listdir` will look in the current directory. It lists all the Jupyter notebooks we're using for the course, as well as the `data` subdirectory. We could find out what's in the `data` subdirectory by looking in `'./data'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['products.csv',\n",
       " 'bad_csv.csv',\n",
       " 'customers.csv',\n",
       " 'orders.csv',\n",
       " 'sample.txt',\n",
       " 'PEP_2016_PEPANNRES.csv',\n",
       " 'yelp.json.gz',\n",
       " 'csv_sample.txt']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('./data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we wanted to find all the files and subdirectories below a directory somewhere on our computer? With `listdir` we only see the files and subdirectories under the particular directory we're looking in. We cannot use `listdir` to automatically search through subdirectories. For this we need to use `walk`, which \"walks\" through all the subdirectories below our chosen directory. We won't cover `walk` in this course, but it's one of the very useful tools (along with the `os.path` sub-module) for working with files in Python, particularly if you are working with many different data files at once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSV files\n",
    "\n",
    "One of the simplest and most common formats for saving data is as comma-separated values (CSV)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/csv_sample.txt', 'r') as f:\n",
    "    csv = f.read()\n",
    "\n",
    "print(csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This format is often used to represent tables of data. Usually a CSV will have rows (separated by newline characters, `'\\n'`) and columns (separated by commas). Otherwise they are no different from any other text file. We can use the special formatting of a CSV to create a list of lists representing the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_table = []\n",
    "with open('./data/csv_sample.txt', 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        list_table.append(line.strip().split(','))\n",
    "\n",
    "list_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we can work with tabular data much more easily in a Pandas DataFrame. Pandas provides a `read_csv` method to read the data directly into a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./data/csv_sample.txt', index_col=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `read_csv` method is very flexible to deal with the formatting of different data sets. Some data sets will include column headers while others may not. Some data sets will include an index while others may not. Some data sets may have values separated by tabs, semicolons, or other characters instead of commas. There are options in the `read_csv` method for dealing with all of these. You can read about them in the [Pandas documentation on `read_csv`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html). We'll also discuss it further in the [Pandas notebook](DS_Pandas.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an example of downloading\n",
    "# and importing real data using `read_csv`\n",
    "\n",
    "if 'factbook.csv' not in os.listdir('./data/'):\n",
    "    !wget -P ./data/ https://perso.telecom-paristech.fr/eagan/class/igr204/data/factbook.csv\n",
    "\n",
    "countries = pd.read_csv('./data/factbook.csv', delimiter=';', skiprows=[1])\n",
    "countries.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also use pandas to write CSV\n",
    "# using the DataFrame's to_csv method\n",
    "\n",
    "pd.DataFrame({'a': [0, 3, 10], 'b': [True, True, False]}).to_csv('./data/pd_write.csv')\n",
    "\n",
    "pd.read_csv('./data/pd_write.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes a CSV won't be perfect. For example, maybe different rows have different numbers of commas. This makes it difficult to interpret the contents of the file as a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the 3rd line only has 2 \"columns\"\n",
    "\n",
    "!cat ./data/bad_csv.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what happens if we try to read this\n",
    "# into a DataFrame using read_csv?\n",
    "\n",
    "pd.read_csv('./data/bad_csv.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas' `read_csv` method will do its best to construct a table out of a poorly formatted CSV, but it may make mistakes. For example, 54 was interpreted as a name instead of as an age, because there were only 2 columns in that line of the file. Data sets will often contain mistakes like bad formatting, missing data, or typos.\n",
    "\n",
    "**Question:** How could we fix the badly formatted CSV so it would work with `read_csv`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON\n",
    "\n",
    "JSON stands for JavaScript Object Notation. JavaScript is a common language for creating web applications, and JSON files are used to collect and transmit information between JavaScript applications. As a result, a lot of data on the internet exists in the JSON file format. For example, Twitter and Google Maps use JSON.\n",
    "\n",
    "A JSON file is essentially a data structure built out of nested dictionaries and lists. Let's make our own example and then we'll examine an example downloaded from the internet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book1 = {'title': 'The Prophet',\n",
    "         'author': 'Khalil Gibran',\n",
    "         'genre': 'poetry',\n",
    "         'tags': ['religion', 'spirituality', 'philosophy', 'Lebanon', 'Arabic', 'Middle East'],\n",
    "         'book_id': '811.19',\n",
    "         'copies': [{'edition_year': 1996,\n",
    "                     'checkouts': 486,\n",
    "                     'borrowed': False},\n",
    "                    {'edition_year': 1996,\n",
    "                     'checkouts': 443,\n",
    "                     'borrowed': False}]\n",
    "         }\n",
    "         \n",
    "book2 = {'title': 'The Little Prince',\n",
    "         'author': 'Antoine de Saint-Exupery',\n",
    "         'genre': 'children',\n",
    "         'tags': ['fantasy', 'France', 'philosophy', 'illustrated', 'fable'],\n",
    "         'id': '843.912',\n",
    "         'copies': [{'edition_year': 1983,\n",
    "                     'checkouts': 634,\n",
    "                     'borrowed': True,\n",
    "                     'due_date': '2017/02/02'},\n",
    "                    {'edition_year': 2015,\n",
    "                     'checkouts': 41,\n",
    "                     'borrowed': False}]\n",
    "         }\n",
    "\n",
    "library = [book1, book2]\n",
    "library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have two books in our `library`. Both books have some common properties: a title, an author, an id, and tags. Each book can have several tags, so we store that data as a list. Additionally, there can be multiple copies of each book, and each copy also has some unique information like the year it was printed and how many times it's been checked out. Notice that if a book is checked out, it also has a due date. It's convenient to store the information about the multiple copies as a list of dictionaries within the dictionary about the book, because every copy shares the same title, author, etc.\n",
    "\n",
    "This structure is typical of JSON files. It has the advantage of reducing redundancy of data. We only store the author and title once, even though there are multiple copies of the book. Also, we don't store a due date for copies that aren't checked out.\n",
    "\n",
    "If we were to put this data in a table, we would have to duplicate a lot of information. Also, since only one copy in our library is checked out, we also have a column with a lot of missing data.\n",
    "\n",
    "|index|title|author|id|genre|tags|edition_year|checkouts|borrowed|due_date|\n",
    "|:---:|:---:|:----:|::|:---:|:--:|:----------:|:-------:|:------:|:------:|\n",
    "|0|The Prophet|Khalil Gibran|811.19|poetry|religion, spirituality, philosophy, Lebanon, Arabic, Middle East|1996|486|False|Null|\n",
    "|1|The Prophet|Khalil Gibran|811.19|poetry|religion, spirituality, philosophy, Lebanon, Arabic, Middle East|1996|443|False|Null|\n",
    "|2|The Little Prince|Antoine de Saint-Exupery|843.912|children|fantasy, France, philosophy, illustrated, fable|1983|634|True|2017/02/02|\n",
    "|3|The Little Prince|Antoine de Saint-Exupery|843.912|children|fantasy, France, philosophy, illustrated, fable|2015|41|False|Null|\n",
    "\n",
    "This is very wasteful. Since JSON files are meant to be shared quickly over the internet, it is important that they are small to reduce the amount of resources needed to store and transmit them.\n",
    "\n",
    "We can write our `library` to disk using the `json` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('./data/library.json', 'w') as f:\n",
    "    json.dump(library, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ./data/library.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/library.json', 'r') as f:\n",
    "    reloaded_library = json.load(f)\n",
    "\n",
    "reloaded_library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that if we loaded it in without JSON\n",
    "# the file would be interpreted as plain text\n",
    "\n",
    "with open('./data/library.json', 'r') as f:\n",
    "    library_string = f.read()\n",
    "\n",
    "# this isn't what we want\n",
    "library_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas can also read_json\n",
    "# notice how it constructs the table\n",
    "# does it represent the data well?\n",
    "\n",
    "pd.read_json('./data/library.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and to_json\n",
    "df.to_json('./data/example_df.json')\n",
    "\n",
    "!head ./data/example_df.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can download JSON files many ways. Sometimes we will download it manually, but we can also use `wget` like we did for the CSV example. Often we'll connect to a website's API which will respond using JSON.\n",
    "\n",
    "Panda's `read_json` method is capable of connecting directly to a URL (whether it's the address of a JSON file or an API connection) and reading the JSON without saving the file to our computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_json('https://api.github.com/repos/pydata/pandas/issues?per_page=5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compressed files (Gzip)\n",
    "\n",
    "Another way we save storage and network resources is by using **compression**. Many times data sets will contain patterns that can be used to reduce the amount of space needed to store the information.\n",
    "\n",
    "A simple example is the following list of numbers: 10, 10, 10, 2, 3, 3, 3, 3, 3, 50, 50, 1, 1, 50, 10, 10, 10, 10\n",
    "\n",
    "Rather than writing out the full list of numbers (18 integers), we can represent the same information with only 14 numbers: (3, 10), (1, 2), (5, 3), (2, 50), (2, 1), (1, 50), (4, 10)\n",
    "\n",
    "Here the first number in each pair is the number of repetitions, and the second number in the pair is the actual value. We've successfully reduced the amount of numbers we need to represent the same data. Most forms of compression use a similar idea, although actual implementations are usually more complex.\n",
    "\n",
    "In the world of data science, the most common compression is Gzip (which uses the [deflate algorithm](http://www.infinitepartitions.com/art001.html)). Gzip files end with the extension `.gz`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -P ./data/ https://archive.org/stream/TheEpicofGilgamesh_201606/eog_djvu.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "\n",
    "with open('./data/eog_djvu.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "with gzip.open('./data/eog_djvu.txt.gz', 'wb') as f:\n",
    "    f.write(text.encode('utf-8'))\n",
    "\n",
    "!ls -lh ./data/eog*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We were able to compress the text of The Epic of Gilgamesh to a third of its original size! Remember that compression depends on patterns in the data. Language has a lot of patterns, but what would happen if we scrambled all the letters in the text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "with gzip.open('./data/eog_djvu_scrambled.txt.gz', 'wb') as f:\n",
    "    f.write(np.random.permutation(list(text)))\n",
    "\n",
    "!ls -lh ./data/eog*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scrambled version only compressed to two-thirds the size of the original. Compression won't perform very well on random data. Compression also doesn't work very well on data that is already small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_text = 'Hello'\n",
    "\n",
    "with open('./data/short_text.txt', 'w') as f:\n",
    "    f.write(short_text)\n",
    "\n",
    "with gzip.open('./data/short_text.txt.gz', 'wb') as f:\n",
    "    f.write(short_text.encode('utf-8'))\n",
    "\n",
    "!ls -lh ./data/short_text*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The compressed file is bigger than the plain text! That's because the compressed file includes a header, which takes up a small amount of extra space. Also, since the text is so short, it's not possible to use patterns to represent the text more efficiently. Therefore we usually save compression for large files.\n",
    "\n",
    "You may have noticed that when we write Gzip files, we have been using a `'wb'` flag instead of a plain `'w'` flag. This is because Gzip is not plain text. When compressing the file we write _binary_ files. The files are not readable as plain text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have to uncompress the file\n",
    "# before we can read it\n",
    "\n",
    "!cat ./data/short_text.txt.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should only use `'w'` for plain text files (which includes CSV and JSON). Using `'w'` instead of `'wb'` for Gzip files, or other files which are not plain text (e.g. images), could damage the file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serialization (`pickle`)\n",
    "\n",
    "Often we will want to save our work in Python and come back to it later. However, that work might be a machine learning model or some other complex object in Python. How do we save complex Python objects? Python has a module for this purpose called `pickle`. We can use `pickle` to write a binary file that contains all the information about a Python object. Later we can load that pickle file and reconstruct the object in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_example = ['hello', {'a': 23, 'b': True}, (1, 2, 3), [['dogs', 'cats'], None]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%expect_exception TypeError\n",
    "\n",
    "# we can't save this as text\n",
    "with open('./data/pickle_example.txt', 'w') as f:\n",
    "    f.write(pickle_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# we can save it as a pickle\n",
    "with open('./data/pickle_example.pkl', 'wb') as f:\n",
    "    pickle.dump(pickle_example, f)\n",
    "\n",
    "with open('./data/pickle_example.pkl', 'rb') as f:\n",
    "    reloaded_example = pickle.load(f)\n",
    "\n",
    "reloaded_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the reloaded example is the same as the original\n",
    "\n",
    "reloaded_example == pickle_example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pickle is an important tool for data scientists. Data processing and training machine learning models can take a long time, and it is useful to save checkpoints.\n",
    "\n",
    "Pandas also has `to_pickle` and `read_pickle` methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NumPy file formats\n",
    "\n",
    "NumPy also has methods for saving and loading data. They are straightforward to use. You may encounter these when working with certain machine learning libraries that require data be stored in NumPy arrays. NumPy arrays are also often used when working with image data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_array = np.random.random((4, 4))\n",
    "print(sample_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to save as plain text\n",
    "np.savetxt('./data/sample_array.txt', sample_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ./data/sample_array.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.loadtxt('./data/sample_array.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to save as compressed binary\n",
    "np.save('./data/sample_array.npy', sample_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ./data/sample_array.npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.load('./data/sample_array.npy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topics used by not discussed:\n",
    "- BASH commands (!)\n",
    "- `wget`\n",
    "- `str.split()`\n",
    "- APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright &copy; 2020 The Data Incubator.  All rights reserved.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "nbclean": true
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
