Welcome to the forth week of the "How to Win a Data Science Competition" course. Here is a short summary of what you will learn.

* *Dmitry Ulyanov *will start this week with **hyperparameter optimization**. We will understand hyperparameter tuning process in general, list most important hyperparameters for major machine learning models and describe their impact. We will also have a special video with **practical tips and tricks**, recorded by four instructors.

* *Mikhail Trofimov *and* Dmitry Altukhov *will discuss **advanced features** that we can extract from the data. We will describe matrix factorization technique for feature extraction, learn to create features based on t-SNE, review concept of feature interactions, and learn to make up new features based on statistics and nearest neighbors. If you fill that you can do more we recommend you to take a look at Honors track programming assignment, where you will need to implement features based on nearest neighbors, which are often could provide an edge in a competition.

* *Marios Michailidis*, kaggle top-1,* *will review **ensembling methods**. We will describe and compare ensembling methods such as weighted averaging, bagging, boosting, stacking. We will outline plan of validation schemes, discuss practical tips and tricks, and implement ensembling in the programming assignment.

To keep up the work on the final project, apply ideas we will discuss and improve your current solution by optimizing your hyperparameters, maybe implementing some of the advanced features and for sure trying out ensembling.

Now let's go ahead and get started!